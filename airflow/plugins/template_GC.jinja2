from datetime import timedelta, datetime, date
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.hooks.S3_hook import S3Hook
from boto3.exceptions import S3UploadFailedError
from botocore.exceptions import ClientError
import logging
from pathlib import Path
import pandas as pd

#Setting information loggers
logging.basicConfig(
    level=logging.info,
    format='%(asctime)s - %(module)s - %(message)s',
    datefmt='%Y-%m-%d')


default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,                                
    'retry_delay': timedelta(minutes=5)
}

#Setting file paths
sql_path= Path(__file__).resolve().parents[1]
sql_name= '{{ sql_name }}'
path_txt=(f'./OT301-python/airflow/datasets/{sql_name}_process.txt')

#Setting the extraction task
def extraccion():
    with open(f'{sql_path}/include/{sql_name}.sql', 'r') as f:
        query = f.read()
        

    try:
        pg_hook = PostgresHook(postgres_conn_id='{{ postgres_conn_id }}')
        logging.info
        (f"-Exporting {sql_name}.")
        df=pg_hook.get_pandas_df(query)
        df.to_csv(f'./OT301-python/airflow/files/{sql_name}_select.csv', sep=',')
    except:
        logging.warning
        (f"-Exporting {sql_name} did not perform as expected.")
        

    
    
#Transformation task, to set
def transformacion():
        
#Reading .csv files
    try:
        df_0 = pd.read_csv(f'./OT301-python/airflow/files/{sql_name}_select.csv', sep=',')
        
        #Dropping unnecessary columns
        df_0 = df_0.drop(['Unnamed: 0'], axis=1)
        
        #Modifying columns according to the issue
        df_0['last_name']=df_0['last_name'].str.replace('_',' ').str.rstrip().str.lstrip()
        df_0['email']=df_0['email'].str.lower().str.rstrip().str.lstrip()
        df_0['career']=df_0['career'].str.replace('_',' ').str.rstrip().str.lstrip()
        df_0['university']= df_0['university'].str.replace('_',' ').str.rstrip().str.lstrip()
        df_0['inscription_date']=pd.to_datetime(df_0['inscription_date']).dt.strftime('%Y-%m-%d').astype(str)
        df_0['inscription_date']= df_0['inscription_date'].str.replace('-', '/')
        df_0['gender']=df_0['gender'].replace({'f':'female','m':'male'})
        
        if df_0.location[0] != '':
            df_0['location'] = df_0['location'].str.lower()

        #Getting the missing columns from codigos_postales.csv
        try:
            
            df_cp = pd.read_csv(f'{{ sql_path }}/assets/codigos_postales.csv')
            df_cp.localidad = df_cp.localidad.str.lower()
            
            if df_cp.postal_code[1]== -1:
                df_cp = df_cp.drop_duplicates(subset=['localidad'],keep='first')
            
                count = 0
                for x in df_0.location:
                    index_df2 = df_cp.index[df_cp['localidad'] == x]
                    df_0.postal_code[count] = df_cp.codigo_postal[index_df2]
                    count = count + 1
            
            else:
                count = 0
                for x in df_0.postal_code:
                    index_df2 = df_cp.index[df_cp['codigo_postal'] == x][0]
                    df_0.location[count] = df_cp.localidad[index_df2]
                    count = count + 1
                
            
        except:
            logging.info(f'({df_cp}) could not be loaded')
            pass


        #Age calculation for people over 18 years old. 
        df_0['age']= pd.to_datetime(df_0['inscription_date']) - pd.to_datetime(df_0['birth_dates'])
        df_0['age']= df_0['age'].astype(int)
        df_0['age']= (df_0['age'] / (10**9) / 3600 / 24 /365.2425).astype(int)
        df_0['age']= df_0.age.apply(lambda age: age + 0 if (age < 0) else age+18)
        #Above, the lambda function is executed, and in this table, if the value is negative, gets it positive to work with it.

        df_0.to_csv(f'./OT301-python/airflow/datasets/{sql_name}_process.txt', sep=',')
    except:
        logging.error
        (f"-File not found.")
        

#Loading task

def cargando(filename: str, key: str, bucket_name: str) -> None:
    try:
        hook = S3Hook('{{ aws_conn_id }}')
        hook.load_file(filename=filename, key=key, bucket_name=bucket_name, replace=True) 
       
    except S3UploadFailedError:
        logging.error('Bucket destination does not exist. Please check its name either on aws or in the code.')
    except ClientError:
        logging.error('Error connecting to Bucket. Check for Admin->Connections->aws_s3_bucket Key and SecretKey loaded data.')

with DAG(
    dag_id=f'DD_{sql_name}_dag_etl',
    default_args= default_args,
    description= '{{ description }}',
    schedule_interval= timedelta (hours=1),     
    start_date= datetime.fromisoformat('2022-09-20'), 
    catchup=False
    ) as dag:

#Ejecucion de tareas
    extraccion_task = PythonOperator(dag=dag, task_id='extraccion', python_callable= extraccion)
    transformacion_task = PythonOperator(dag=dag, task_id='transformacion', python_callable= transformacion)
    cargando_task = PythonOperator(dag=dag, task_id='cargando', python_callable=cargando,
    op_kwargs={
        'filename':f'OT301-python/airflow/datasets/{sql_name}_process.txt',
        'key':f'{sql_name}_process.txt',
        'bucket_name':'cohorte-septiembre-5efe33c6'}) 


    extraccion_task >> transformacion_task >> cargando_task