{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP 10 most viewed posts.\n",
    "\n",
    "from xml.etree.cElementTree import ElementTree as ET\n",
    "from functools import reduce\n",
    "from typing import Counter\n",
    "import re\n",
    "import os\n",
    "import operator\n",
    "import csv\n",
    "\n",
    "def chunkify(iterable,len_of_chunk):\n",
    "    for i in range(0,len(iterable),len_of_chunk):\n",
    "        yield iterable[i:i + len_of_chunk]\n",
    "\n",
    "def get_title_and_views(data):\n",
    "    id=data.attrib['Id'] #NO BORRAR\n",
    "    views=data.attrib['ViewCount']\n",
    "    return views, id,  \n",
    "\n",
    "def mapeado(data):    \n",
    "    post_mapeados= list(map(get_title_and_views, data))\n",
    "    return post_mapeados\n",
    "\n",
    "def order_data(post_mapeado):\n",
    "    #ordered_data=post_mapeado.sort(key=operator.itemgetter(1))\n",
    "    int_tuple=tuple(tuple(map(int, x)) for x in post_mapeado)\n",
    "    ordered_data=sorted(int_tuple, key=operator.itemgetter(0), reverse=True) # poner 0 en el parentesis \n",
    "    return ordered_data                                                      # si vuelvo a poner la ID\n",
    "                                                                             # mas arriba\n",
    "\n",
    "def max_value(top_10_post):\n",
    "    return max(top_10_post)\n",
    "\n",
    "def unify_chunks(data1,data2):\n",
    "    for tupla in data2:\n",
    "        data1.append(tupla)\n",
    "    return data1\n",
    "\n",
    "#current directory(notebooks)\n",
    "current_folder=os.path.dirname(os.path.realpath(__file__))\n",
    "#previous directory(bigdata)\n",
    "bigdata_path=os.path.abspath(os.path.join(current_folder, os.pardir))\n",
    "#datasets path\n",
    "datasets_path=bigdata_path+'/datasets/'\n",
    "tree=ET()\n",
    "tree.parse(f'{datasets_path}/posts.xml')\n",
    "root=tree.getroot()\n",
    "data_chunks=chunkify(root,50)\n",
    "post_mapeado=list(map(mapeado,data_chunks))\n",
    "\n",
    "def set_int_tuple(unify_list):\n",
    "    lista_aux =[]\n",
    "    for dato in unify_list:\n",
    "        lista_aux.append((int(dato[0]),int(dato[1])))\n",
    "    return lista_aux\n",
    "\n",
    "unify_list=(reduce(unify_chunks,post_mapeado))\n",
    "\n",
    "int_list=set_int_tuple(unify_list)\n",
    "\n",
    "def order_list(int_list):\n",
    "    ordered_list=sorted(int_list, key=operator.itemgetter(0), reverse=True)\n",
    "    return ordered_list\n",
    "\n",
    "ordered_list=order_list(int_list)\n",
    "\n",
    "top_10=(ordered_list[0:10])\n",
    "\n",
    "print(top_10)\n",
    "\n",
    "output=zip(top_10)\n",
    "output_path=bigdata_path+'/outputs/'\n",
    "\n",
    "with open(f'{output_path}GG_top_10_most_viewed_posts.csv','w',newline='') as file:\n",
    "    csv_out=csv.writer(file)\n",
    "    csv_out.writerow(['Post_views','Post_id'])\n",
    "    for row in top_10:\n",
    "        csv_out.writerow(row)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "height=[item0[0] for item0 in top_10]\n",
    "tick_label=[item1[1] for item1 in top_10]\n",
    "\n",
    "left=[1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "plt.bar(x=left,height=height,tick_label=tick_label,width=0.5,color=['grey'])\n",
    "plt.xlabel('Posts Id')\n",
    "plt.ylabel('Views quantity')\n",
    "plt.title('TOP 10 most viewed posts.')\n",
    "plt.show()\n",
    "\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average response time for top 200-300 most scored posts.\n",
    "\n",
    "from xml.etree.cElementTree import ElementTree as ET\n",
    "from functools import reduce\n",
    "from typing import Counter\n",
    "import os\n",
    "import operator\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "def chunkify(iterable,len_of_chunk):\n",
    "    for i in range(0,len(iterable),len_of_chunk):\n",
    "        yield iterable[i:i + len_of_chunk]\n",
    "\n",
    "#current directory(notebooks)\n",
    "current_folder=os.path.dirname(os.path.realpath(__file__))\n",
    "#previous directory(bigdata)\n",
    "bigdata_path=os.path.abspath(os.path.join(current_folder, os.pardir))\n",
    "#datasets path\n",
    "datasets_path=bigdata_path+'/datasets/'\n",
    "\n",
    "tree=ET()\n",
    "tree.parse(f'{datasets_path}/posts.xml')\n",
    "root=tree.getroot()\n",
    "data_chunks1=chunkify(root,50)\n",
    "data_chunks2=chunkify(root,50)\n",
    "\n",
    "date_format='%Y-%m-%dT%H:%M:%S.%f'\n",
    "\n",
    "def question_posts(data):\n",
    "    score=data.attrib['Score']\n",
    "    int_score=int(score)\n",
    "    post_id=data.attrib['Id']\n",
    "    post_type_id=int(data.attrib['PostTypeId'])\n",
    "    creation_date=data.attrib['CreationDate']\n",
    "    try:\n",
    "        accepted_answ_id=data.attrib['AcceptedAnswerId']\n",
    "    except:\n",
    "        return\n",
    "    return  accepted_answ_id,int_score,post_id,creation_date,post_type_id\n",
    "\n",
    "def answer_posts(data):\n",
    "    post_id=data.attrib['Id']\n",
    "    post_type_id=int(data.attrib['PostTypeId'])\n",
    "    creation_date=data.attrib['CreationDate']\n",
    "    return  post_id,creation_date,post_type_id\n",
    "\n",
    "def prueba(data):\n",
    "    if data[4]=='1':\n",
    "        return data[4]\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def mapeado_question(data):    \n",
    "    post_mapeados= list(map(question_posts, data))\n",
    "    post_mapeados=list(filter(None,post_mapeados))\n",
    "    post_mapeados=list(filter(lambda x:x[4]==1,post_mapeados))\n",
    "    return list(post_mapeados)\n",
    "\n",
    "def mapeado_answer(data):    \n",
    "    answer_mapeado= list(map(answer_posts, data))\n",
    "    answer_mapeado=filter(None,answer_mapeado)\n",
    "    answer_mapeado=list(filter(lambda x:x[2]==2,answer_mapeado))\n",
    "    return list(answer_mapeado)\n",
    "\n",
    "question_mapped=list(map(mapeado_question,data_chunks1))\n",
    "answer_mapped=list(map(mapeado_answer,data_chunks2))\n",
    "\n",
    "def unify(data1,data2):\n",
    "    for tupla in data2:\n",
    "        data1.append(tupla)\n",
    "    return data1\n",
    "\n",
    "question_unified=reduce(unify,question_mapped)\n",
    "answer_unified=reduce(unify,answer_mapped)\n",
    "\n",
    "def merged(question,answer):\n",
    "    id = operator.itemgetter(0)\n",
    "    answ_info = {id(post_id): post_id[1:] for post_id in answer}\n",
    "    merged = [quest_id + answ_info[id(quest_id)] for quest_id in question if id(quest_id) in answ_info]\n",
    "    return merged\n",
    "\n",
    "merged_list=merged(question_unified,answer_unified)\n",
    "\n",
    "def order_list(data):\n",
    "    ordered_list=sorted(data, key=operator.itemgetter(1), reverse=True)\n",
    "    return ordered_list\n",
    "\n",
    "ordered_merged_list=order_list(merged_list)\n",
    "top_200_300_first=ordered_merged_list[200:300]\n",
    "\n",
    "question_dates_string=[x[3] for x in top_200_300_first]\n",
    "\n",
    "def set_date(data):\n",
    "    lista_aux =[]\n",
    "    for dato in data:\n",
    "        correct_Date=datetime.strptime(dato,date_format)\n",
    "        lista_aux.append(correct_Date)\n",
    "    return lista_aux\n",
    "\n",
    "answer_dates_string=[x[5] for x in top_200_300_first]\n",
    "\n",
    "question_dates = [datetime.strptime(date, date_format) for date in question_dates_string]\n",
    "\n",
    "answer_dates=[datetime.strptime(date, date_format) for date in answer_dates_string]\n",
    "\n",
    "operator_dif=list(map(operator.sub,answer_dates,question_dates))\n",
    "\n",
    "quest=list(pd.to_datetime(question_dates_string))\n",
    "answ=list(pd.to_datetime(answer_dates_string))\n",
    "\n",
    "pandas_dif=list(map(operator.sub,answ,quest))\n",
    "\n",
    "\n",
    "total_hours=(sum(operator_dif,timedelta()))\n",
    "total_seconds=(total_hours.total_seconds())\n",
    "average_response_time_in_hours=(total_seconds/100)/3600\n",
    "\n",
    "print('Average response time:',average_response_time_in_hours,' hours')\n",
    "\n",
    "output=f'Average response time: {average_response_time_in_hours} hours.'\n",
    "output_path=bigdata_path+'/outputs/'\n",
    "\n",
    "with open(f'{output_path}GG_200_300_scored_avg_resp_time.csv','w') as file:\n",
    "    file.write(str(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP 10 most mentioned words per tag.\n",
    "\n",
    "from xml.etree.cElementTree import ElementTree as ET\n",
    "from functools import reduce\n",
    "from typing import Counter\n",
    "import re\n",
    "import os\n",
    "\n",
    "from numpy import dtype\n",
    "\n",
    "def chunkify(iterable,len_of_chunk):\n",
    "    for i in range(0,len(iterable),len_of_chunk):\n",
    "        yield iterable[i:i + len_of_chunk]\n",
    "\n",
    "def get_tags_and_body(data):\n",
    "    try:\n",
    "        tags=data.attrib['Tags']\n",
    "    except:\n",
    "        return\n",
    "    body=data.attrib['Body']\n",
    "    body=re.sub('<[^<]+?>', '', body)\n",
    "    tags=re.findall('<(.+?)>',tags)\n",
    "    words_count=Counter(body.split())\n",
    "    return tags, words_count\n",
    "\n",
    "def separar_tags_y_palabras(data):\n",
    "    return dict([tag, data[1].copy()]for tag in data[0])\n",
    "\n",
    "def reducir_contadores(data1,data2):\n",
    "    for key,value in data2.items():\n",
    "        if key in data1.keys():\n",
    "            data1[key].update(data2[key])\n",
    "        else:\n",
    "            data1.update({key: value})\n",
    "    return data1\n",
    "\n",
    "def mapeado(data_chunks):    \n",
    "    post_mapeados= list(map(get_tags_and_body, data_chunks))\n",
    "    cleaned_map=list(filter(None,post_mapeados))\n",
    "    words_per_tag=list(map(separar_tags_y_palabras,cleaned_map))\n",
    "    try:\n",
    "        reduced_list=reduce(reducir_contadores,words_per_tag)\n",
    "    except:\n",
    "        return\n",
    "    return reduced_list\n",
    "\n",
    "def unify_chunks(data1,data2):\n",
    "    for tupla in data2:\n",
    "        data1.append(tupla)\n",
    "    return data1\n",
    "\n",
    "def calculate_top_10(data):\n",
    "    return data[0], data[1].most_common(10)\n",
    "\n",
    "#current directory(notebooks)\n",
    "current_folder=os.path.dirname(os.path.realpath(__file__))\n",
    "#previous directory(bigdata)\n",
    "bigdata_path=os.path.abspath(os.path.join(current_folder, os.pardir))\n",
    "#datasets path\n",
    "datasets_path=bigdata_path+'/datasets/'\n",
    "\n",
    "tree=ET()\n",
    "tree.parse(f'{datasets_path}/posts.xml')\n",
    "root=tree.getroot()\n",
    "data_chunks=chunkify(root,50)\n",
    "mapped_list=list(map(mapeado,data_chunks))\n",
    "mapped_list=list(filter(None,mapped_list))\n",
    "reduced_list=reduce(reducir_contadores,mapped_list)\n",
    "top_10=dict(map(calculate_top_10,reduced_list.items()))\n",
    "\n",
    "output_path=bigdata_path+'/outputs/'\n",
    "int_to_str = lambda int_value: str(int_value)\n",
    "\n",
    "with open(f'{output_path}GG_top_10_words_per_tag.csv','w',encoding='utf-8') as file:\n",
    "    for key, values in top_10.items():\n",
    "         for tuple_value in values:\n",
    "             csv_row = [key] + list(map(int_to_str, list(tuple_value)))\n",
    "             file.write(', '.join(csv_row)+'\\n')\n",
    "\n",
    "print(top_10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9f5fc863bfeb52628e16391e02b82346ddba486f3a90a174bf61c42516759ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
