{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' OT301-105\n",
    "COMO: Analista de datos\n",
    "QUIERO: Utilizar MapReduce\n",
    "PARA: Analizar los datos de StackOverflow\n",
    "\n",
    "Criterios de aceptación: \n",
    "\n",
    "Top 10 fechas con mayor cantidad de post creados\n",
    "\n",
    "Relación entre cantidad de respuestas y sus visitas.\n",
    "\n",
    " Del ranking de los primeros 0-100 por score, tomar el tiempo de respuesta promedio e informar un único valor.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from typing import Counter\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from os import remove\n",
    "from datetime import date, datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables and functions for the notebook\n",
    "root_path = os.path.abspath('').replace('notebooks','datasets')\n",
    "outputs_path = os.path.abspath('').replace('notebooks','outputs')\n",
    "file_xml = os.path.join(root_path, 'posts.xml')\n",
    "tree = ET.parse(file_xml)\n",
    "root = tree.getroot()\n",
    "\n",
    "# splits into iterable subsets to process separately\n",
    "def chunckify(iterable, len_of_chunk):\n",
    "    for i in range(0, len(iterable), len_of_chunk):\n",
    "        yield iterable[i:i + len_of_chunk]\n",
    "\n",
    "# unifies many lists of tuples into only one\n",
    "def unify_list(data1,data2):\n",
    "    for data in data2:\n",
    "        data1.append(data)\n",
    "    return data1\n",
    "\n",
    "# Function that generates all external files generated by the project\n",
    "def outputs(data, name,title):\n",
    "    file_name = os.path.join(outputs_path, name)\n",
    "    try:\n",
    "        remove(file_name)\n",
    "    except:\n",
    "        pass\n",
    "    if name[-4:] == '.csv':\n",
    "        with open(file_name, 'w') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(title)\n",
    "            if type(data) == list:\n",
    "                writer.writerows(data)\n",
    "            elif type(data) == dict:\n",
    "                for key,value in data.items():\n",
    "                    writer.writerow([key,value])\n",
    "    elif name[-4:] == '.txt':\n",
    "        with open(file_name, 'w') as file:\n",
    "            file.write(title)\n",
    "            file.write(str(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions corresponding only to the first analysis\n",
    "# returns the information needed for analysis\n",
    "def get_date_and_type(data):\n",
    "    if data.attrib['PostTypeId'] == '1':\n",
    "        return data.attrib['CreationDate'][0:10]\n",
    "    else:\n",
    "        return\n",
    "\n",
    "# function create a map that stores the creation dates and the creation dates for each batch of fragments\n",
    "def mapper(data):\n",
    "    type_date = list(map(get_date_and_type, data))\n",
    "    type_date = list(filter(None,type_date))\n",
    "    counter_dates = dict(Counter(type_date))\n",
    "    return dict(counter_dates)\n",
    "\n",
    "# accumulates the frequency of occurrence of the creation date and merge in one list\n",
    "def date_acumulator(data1, data2):\n",
    "    for key, value in data2.items():\n",
    "        if key in data1.keys():\n",
    "            data1[key] += value\n",
    "        else:\n",
    "            data1[key] = value\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 dates with the highest number of posts created, I assume that the post is generated with the questions\n",
    "def top_10_creation_post_dates():\n",
    "    data_chunks = chunckify(root,50)\n",
    "    mapped = list(map(mapper, data_chunks))\n",
    "    reduced_mapped = reduce(date_acumulator, mapped)\n",
    "    outputs(reduced_mapped, 'GE_E01_creation_date_frecuency.csv',['creation_date', 'frequency'])\n",
    "    top_10 = Counter(reduced_mapped).most_common(10)\n",
    "    outputs(top_10, 'GE_E01_top10_creation_date.csv',['creation_date', 'frequency'])\n",
    "\n",
    "# top_10_creation_post_dates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions corresponding only to the second analysis\n",
    "# returns the information needed for analysis\n",
    "def get_answer_visit(data):\n",
    "    try:\n",
    "        return int(data.attrib['ViewCount']), int(data.attrib['AnswerCount'])\n",
    "    except:\n",
    "        return\n",
    "\n",
    "# accumulates the number of visits and responses for each batch of chunk\n",
    "def mapper_answer_visit(data):\n",
    "    answer_visit_data = list(map(get_answer_visit,data))\n",
    "    answer_visit_data = list(filter(None,answer_visit_data))\n",
    "    return answer_visit_data\n",
    "\n",
    "# plots on the cartesian plane the values ​​of the number of visits (x-axis) versus the number of responses to a question\n",
    "# the function allows parameterizing a visit limit\n",
    "def print_cartesian_plane(mapped_list, top_visits = 100000, save=False):\n",
    "    if top_visits == 100000:\n",
    "        plt.title(\"Answers and Visits Relationship\")\n",
    "    else:\n",
    "        plt.title(f\"Answers and Visits Relationship, visit limit {top_visits}\")\n",
    "    plt.xlabel(\"Visits\")\n",
    "    plt.ylabel(\"Answers\")\n",
    "    for point in mapped_list:\n",
    "        if point[0]< top_visits:\n",
    "            plt.plot(point[0],point[1],marker=\".\", color=\"red\")\n",
    "    if save:\n",
    "        if top_visits == 100000:\n",
    "            file_name = f'{outputs_path}/GE_E02_av_relationship.png'\n",
    "        else:\n",
    "            file_name = f'{outputs_path}/GE_E02_av_relationship_top{top_visits}.png'\n",
    "        try:\n",
    "            remove(file_name)\n",
    "        except:\n",
    "            pass\n",
    "        plt.savefig(file_name)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analize the answer vs visit relationship\n",
    "def answer_visit_relationship():\n",
    "    data_chunks = chunckify(root,50)\n",
    "    mapped = list(map(mapper_answer_visit, data_chunks))\n",
    "    mapped_list = reduce(unify_list,mapped)\n",
    "    outputs(mapped_list, 'GE_E02_visits_answers_quantity.csv',['visits', 'answers'])\n",
    "    print_cartesian_plane(mapped_list,1000, save=True)\n",
    "    \n",
    "# answer_visit_relationship()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions corresponding only to the third analysis\n",
    "# returns the questions information needed for analysis\n",
    "def get_question_data(data):\n",
    "    try:\n",
    "        return int(data.attrib['Score']), data.attrib['Id'], data.attrib['AcceptedAnswerId'], data.attrib['CreationDate']\n",
    "    except:\n",
    "        return\n",
    "\n",
    "# function create a map that stores the questions information for each batch of fragments\n",
    "def get_questions_map(data):\n",
    "    questions_map = map(get_question_data,data)\n",
    "    return list(filter(None,questions_map))\n",
    "\n",
    "# returns the answers information needed for analysis\n",
    "def get_answer_data(data):\n",
    "    try:\n",
    "        dato = data.attrib['Id'], data.attrib['ParentId'],data.attrib['CreationDate']\n",
    "        if dato[0] in answer_list:\n",
    "            return dato\n",
    "        else:\n",
    "            return\n",
    "    except:\n",
    "        return\n",
    "\n",
    "# function create a map that stores the answers information for each batch of fragments\n",
    "def get_answers_map(data):\n",
    "    answers_map = map(get_answer_data,data)\n",
    "    return list(filter(None,answers_map))\n",
    "\n",
    "# merge the question and answer lists into a single list\n",
    "def merged(question,answer):\n",
    "    id_a = operator.itemgetter(0)\n",
    "    answ_info = {id_a(post_id): post_id[1:] for post_id in answer}\n",
    "    id_q = operator.itemgetter(2)\n",
    "    # ques_info = {id_q(post_id): (post_id[0],post_id[1],post_id[3]) for post_id in question}\n",
    "    merged = [quest_id + answ_info[id_q(quest_id)] for quest_id in question if id_q(quest_id) in answ_info]\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.2491\n"
     ]
    }
   ],
   "source": [
    "# Del ranking de los primeros 0-100 por score, tomar el tiempo de respuesta promedio e informar un único valor.\n",
    "# Returns the necessary information for the analysis of the questions between two chosen scores (by default it is from 0 to 100)\n",
    "def list_top100_scored_questions(begin=0,end=100):\n",
    "    data_chunks = chunckify(root,50)\n",
    "    questions_map = list(map(get_questions_map, data_chunks))\n",
    "    questions_list = reduce(unify_list, questions_map)\n",
    "    questions_list = sorted(questions_list, key=operator.itemgetter(0), reverse=True)\n",
    "    outputs(questions_list, 'GE_E03_top100_scored_questions.csv',['Score','QuestionId','AcceptedAnswerId','CreationDate'])\n",
    "    return questions_list[begin:end]\n",
    "\n",
    "# returns the average response according to the selected data\n",
    "def average_response_time():\n",
    "    data_chunks = chunckify(root,50)\n",
    "    answer_data_list = list(map(get_answers_map, data_chunks))\n",
    "    answer_data_list = reduce(unify_list, answer_data_list)\n",
    "    merged_list = merged(top100_scored_questions,answer_data_list)\n",
    "\n",
    "    date_format='%Y-%m-%dT%H:%M:%S.%f'\n",
    "    questions_dates_string=[x[3] for x in merged_list]\n",
    "    questions_dates = [datetime.strptime(date, date_format) for date in questions_dates_string]\n",
    "\n",
    "    answers_dates_string=[x[5] for x in merged_list]\n",
    "    answers_dates=[datetime.strptime(date, date_format) for date in answers_dates_string]\n",
    "    \n",
    "    operator_dif=list(map(operator.sub,answers_dates,questions_dates))\n",
    "\n",
    "    total = sum(operator_dif, timedelta())\n",
    "    total_add_time = (total.days + total.seconds / 3600)\n",
    "    average_response_time_in_days = total_add_time / 100\n",
    "    outputs(average_response_time_in_days, 'GE_E03_average_response_time_in_days.txt','Average response time in days: ')\n",
    "    print(average_response_time_in_days)\n",
    "\n",
    "# top100_scored_questions = list_top100_scored_questions()\n",
    "# answer_list = [x for w, v, x, y in top100_scored_questions]\n",
    "# average_response_time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
